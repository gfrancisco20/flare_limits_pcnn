{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Predictions\n",
    "\n",
    "Notebook to compute and store predictions on the operational test set.\n",
    "\n",
    "Full-disk and patches/sector predictions are stored in separated files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "\n",
    "if COLAB : \n",
    "  configSetup = {\n",
    "      'COLAB'           : 'True',\n",
    "      'PATH_ROOT_DRIVE' : '/content/drive/MyDrive/Projects/Forecast',\n",
    "      'PATH_ROOT_LOCAL' : '/content/session',\n",
    "      'PATH_SUNDL'      : '/content/sundl',\n",
    "      'PATH_PROJECT'    : '/content/flare_limits_pcnn'\n",
    "  }\n",
    "  !git clone https://github.com/gfrancisco20/sundl.git\n",
    "  !git clone https://github.com/gfrancisco20/flare_limits_pcnn.git\n",
    "  import sys\n",
    "  import re\n",
    "  sys.path.append(configSetup['PATH_SUNDL'])\n",
    "  sys.path.append(configSetup['PATH_PROJECT'])\n",
    "  configFile = f'{configSetup[\"PATH_PROJECT\"]}/config.py'\n",
    "  with open(configFile, 'r') as file:\n",
    "    content = file.read()\n",
    "  for constant in configSetup.keys():\n",
    "    content = re.sub(re.compile(f'{constant} = .*'), f'{constant} = \\'{configSetup[constant]}\\'', content)\n",
    "  with open(configFile, 'w') as file:\n",
    "    file.write(content)\n",
    "   \n",
    "from config import *\n",
    "if COLAB:\n",
    "  from sundl.utils.colab import mountDrive\n",
    "  # mouting drive content in session on colab\n",
    "  mountDrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = PATH_RES/'Classification_PCNN_Limits_Paper_2024_03_21__0'# 'Results_Paper_PCNN' #\n",
    "\n",
    "# Seleted Models from the CV resullts\n",
    "modelDict = {\n",
    "  # 'C+_mpf_Persistant_24'                                    : 'C+_Persistant',\n",
    "  # 'C+_mpf_PTx8_RtdXall_ProgPos_AW4e6D4e3_blos_24'           : 'C+_SPCNN_Blos', #\n",
    "  # 'C+_mpf_PTx8_RtdXall_ProgPos_AW1e5D1e4_0193x0211x0094_24' : 'C+_SPCNN_EUV',\n",
    "  # 'M+_mpf_Persistant_24'                                    : 'M+_Persistant',\n",
    "  # 'M+_mpf_PTx8_RtdXall_LowC_AW6e6D1e3_blos_24'              : 'M+_SPCNN_Blos',\n",
    "  # 'M+_mpf_PTx8_RtdXall_ProgPos_AW1e5D1e4_0193x0211x0094_24' : 'M+_SPCNN_EUV',\n",
    "  'C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4' : 'C+_SPCNN_Blos'\n",
    "}\n",
    "modelDictRev = {modelDict[oldName] : oldName for oldName in modelDict.keys()}\n",
    "\n",
    "ensembles = {'C+_SPCNN_Both_Max'  : ['C+_SPCNN_Blos','C+_SPCNN_EUV'],\n",
    "             'M+_SPCNN_Both_Max'  : ['M+_SPCNN_Blos','M+_SPCNN_EUV'], \n",
    "             'C+_SPCNN_Both_Avg'  : ['C+_SPCNN_Blos','C+_SPCNN_EUV'],\n",
    "             'M+_SPCNN_Both_Avg'  : ['M+_SPCNN_Blos','M+_SPCNN_EUV'] \n",
    "            #  'C+_SPCNN_Histo' : ['C+_SPCNN_Blos','C+_SPCNN_EUV','C+_Persistant'],\n",
    "            #  'M+_SPCNN_Histo' : ['M+_SPCNN_Blos','M+_SPCNN_EUV', 'M+_Persistant']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodelName\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelName' is not defined"
     ]
    }
   ],
   "source": [
    "modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C+',\n",
       " 'mpf',\n",
       " 'png',\n",
       " 'blos',\n",
       " '24',\n",
       " '',\n",
       " 'PTx8',\n",
       " 'RtdXall',\n",
       " 'ProgPos',\n",
       " 'AW1e5D1e4']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras',\n",
       " '/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras_config.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsConfigAndPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in modelsConfigAndPath if m[-3:]!='pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras_config.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFoldsPath = [m for m in modelsConfigAndPath if m[-3:]!='pkl']\n",
    "modelFoldsConfigs = [m for m in modelsConfigAndPath if m[-3:]=='pkl']\n",
    "modelFoldsConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFoldsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras',\n",
       " '/Users/greg/Google Drive/Mi unidad/Projects/Forecast/Results/Flare/Classification_PCNN_Limits_Paper_2024_03_21__0/models/C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4_fd000.keras_config.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsConfigAndPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 17:22:55.826629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING : Rmv l20  !\n",
      "\n",
      "\n",
      "MODEL :  C+_mpf_png_blos_24__PTx8_RtdXall_ProgPos_AW1e5D1e4\n",
      "FOLDS WITH MODELS:  ['fd000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Projects/flare_limits_pcnn/../sundl/sundl/dataloader/sdocml.py:193: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ).apply(lambda x: x[-1]).shift(freq = f'-{offLabel}H')#[:-int(offLabel/2)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING : 12 NaN (droped) for label at ts 24\n",
      "------------------------------\n",
      "labels.shape (0,)\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:90\u001b[0m\n",
      "File \u001b[0;32m~/Projects/flare_limits_pcnn/../sundl/sundl/dataloader/sdocml.py:333\u001b[0m, in \u001b[0;36mbuilDS_image_feature\u001b[0;34m(pathDir, channels, batch_size, dfTimeseries, samples, shiftSamplesByLabelOff, shiftTsByLabOff, ts_off_label_hours, ts_off_scalar_hours, labelCol, scalarCol, prefetch, cache, shuffle, uncachedShuffBuff, img_size, crop_coord, num_classes, gray2RGB, chnWiseNormalize, pathNormFile, shape3d, regression, labelEncoder, encoderIsTf, scalarEncoder, weightByClass, weightOffLabIdx, classTresholds, classWeights, strictly_pos_label, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m#######################################################################\u001b[39;00m\n\u001b[1;32m    331\u001b[0m filenames_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(filenames)\n\u001b[0;32m--> 333\u001b[0m im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    334\u001b[0m isGray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(im\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chnWiseNormalize:\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;66;03m# rmv?\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sundl.utils.data import read_Dataframe_With_Dates\n",
    "\n",
    "for modelName in modelDict.keys():\n",
    "  # TBR\n",
    "  try:\n",
    "    if modelName.split('_')[2][3] in ['4']:\n",
    "      # TODO : make generic to patch number.\n",
    "      continue\n",
    "  except:\n",
    "    pass\n",
    "    \n",
    "  labelCol = modelName.split('_')[1]\n",
    "  h = modelName.split('_')[-1]\n",
    "  \n",
    "  #@@@@@@@@\n",
    "  print('WARNING : Rmv l20  !')\n",
    "  h=24\n",
    "  #@@@@@@@@\n",
    "\n",
    "  pathPredFd = F_PATH_PREDS(FOLDER)/f'{modelName}_fd.csv'\n",
    "  pathPredPt = F_PATH_PREDS(FOLDER)/f'{modelName}_pt.csv'\n",
    "  if not pathPredFd.exists():\n",
    "    print('\\n\\nMODEL : ',modelName)\n",
    "\n",
    "    # loading model path and configs for availlable fold models\n",
    "    modelsConfigAndPath = glob((FOLDER/f'models/{modelName}*').as_posix())\n",
    "    modelFoldsPath = [m for m in modelsConfigAndPath if m[-3:]!='pkl']\n",
    "    modelFoldsConfigs = [m for m in modelsConfigAndPath if m[-3:]=='pkl']\n",
    "    # foldIds = [m[-5:] for m in modelFoldsPath]\n",
    "    foldIds = [m.split('/')[-1].split('.')[0][-5:] for m in modelFoldsPath]\n",
    "    n_fold = len(foldIds)\n",
    "    if n_fold == 0:\n",
    "      continue\n",
    "    # for foldIdx in range(n_fold-1,-1,-1):\n",
    "    #   if not Path(f'{modelFoldsPath[foldIdx]}/assets').exists():\n",
    "    #     modelFoldsPath.pop(foldIdx)\n",
    "    #     modelFoldsConfigs.pop(foldIdx)\n",
    "    #     foldIds.pop(foldIdx)\n",
    "    print('FOLDS WITH MODELS: ',foldIds)\n",
    "    if len(foldIds)==0:\n",
    "      continue\n",
    "    with open(modelFoldsConfigs[0], 'rb') as f1:\n",
    "      config = pickle.load(f1)\n",
    "    configTest = config['dataset_val']\n",
    "    dfTest = read_Dataframe_With_Dates(F_PATH_TEST(configTest['labelCol'],configTest['ts_off_label_hours'][0]))\n",
    "    # configTest = config['dataset_val']\n",
    "    configTest['dfId2History'] = dfTest.copy()\n",
    "    configTest['samples'] = None\n",
    "    configTest['cache'] = False\n",
    "    configTest['shuffle'] = False\n",
    "    configTest['weightByClass'] = False\n",
    "    configTest['batch_size'] = 64\n",
    "    configTest['epochs'] = 1\n",
    "    # STOP\n",
    "    # encoders consstantss **\n",
    "    classTresholds = configTest['classTresholds']\n",
    "    binCls = modelName[0]\n",
    "    # **\n",
    "    if modelName.split('_')[2] == 'Persistant':\n",
    "      # dsTest , _, _, _ = build_dataset_persistant(**configTest)\n",
    "      # saved pstmod not working\n",
    "      dfId2History =  dfTest.copy()\n",
    "      ts_off_label_hours = h\n",
    "      ts_off_history_hours = -h\n",
    "      dfId2History.index = dfId2History.index.shift(periods = -ts_off_label_hours, freq='H')\n",
    "      input_lag = - ts_off_history_hours\n",
    "      dfId2History['history'] = dfId2History[labelCol].rolling(window = f'{input_lag}H',\n",
    "                                                      closed = 'right', # min_periods = int(input_lag)\n",
    "                                                      ).apply(\n",
    "                                                          lambda x: x[0]) # we remove first month in case of incomplete windows\n",
    "      dfId2History = dfId2History[int(ts_off_label_hours/2):-int(ts_off_label_hours/2)]\n",
    "      dfId2History = dfId2History.copy()\n",
    "\n",
    "      dfId2History['pred'] = dfId2History['history'].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfId2History['label'] = dfId2History[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfPred = dfId2History[[labelCol,'cls','label','pred']].copy()\n",
    "      dfPred.to_csv(pathPredFd)\n",
    "    else:\n",
    "      if 'buildDsFunction' in config.keys():\n",
    "        buildDs = config[f'buildDsFunction']\n",
    "      else:\n",
    "        # default dataset builder if not stored in config (adapt if needed)\n",
    "        buildDs = builDS_image_feature\n",
    "      dsTest , _, missing_file_regexp, dfSamples_corr = buildDs(**configTest)\n",
    "      dfSamples_corr = dfSamples_corr.set_index('timestamp',drop = True)\n",
    "\n",
    "      # predictionss\n",
    "      predsFd = []\n",
    "      patch_predsFd = []\n",
    "      for foldIdx,modelFoldPath in enumerate(modelFoldsPath):\n",
    "        # Full-disk predictions\n",
    "        model = tf.keras.models.load_model(modelFoldPath, compile=False)\n",
    "        \n",
    "        STOP\n",
    "        \n",
    "        predsFd.append(model.predict(dsTest))\n",
    "        # Patches predictions\n",
    "        if modelName.split('_')[2][:2] == 'PT':\n",
    "          for layer in model.layers:\n",
    "            if layer.name == 'time_distributed':\n",
    "              patchesBlock = layer\n",
    "          patches = tf.keras.Model(model.input, patchesBlock.output, name='patches')\n",
    "          patch_predsFd.append(patches.predict(dsTest))\n",
    "          # predsFd could be retrieved from it to avoid double computation\n",
    "          del patches\n",
    "        del model\n",
    "      del dsTest\n",
    "\n",
    "      # predictions to dataframes\n",
    "      dfPred = dfSamples_corr[[labelCol,'cls']].copy()\n",
    "      dfPred['label'] = dfPred[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfPred['pred'] = np.zeros(len(dfPred))\n",
    "\n",
    "      for idx,foldId in enumerate(foldIds):\n",
    "        dfPred[f'pred_{foldId}'] = predsFd[idx][:,1]\n",
    "        dfPred['pred'] += dfPred[f'pred_{foldId}']\n",
    "      dfPred['pred'] /= (idx+1)\n",
    "      dfPred.to_csv(pathPredFd)\n",
    "\n",
    "      if modelName.split('_')[2][:2] == 'PT':\n",
    "        dfPredPatches = dfSamples_corr[[labelCol,'cls']].copy()\n",
    "        dfPredPatches['label'] = dfPredPatches[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "        num_ptch = patch_predsFd[0].shape[1]\n",
    "        for ptchId in range(num_ptch):\n",
    "          dfPredPatches[f'pred_pt{ptchId}'] = np.zeros(len(dfPredPatches))\n",
    "          for idx,foldId in enumerate(foldIds):\n",
    "            dfPredPatches[f'pred_pt{ptchId}_{foldId}'] = patch_predsFd[idx][:,ptchId,0]\n",
    "            dfPredPatches[f'pred_pt{ptchId}']  += dfPredPatches[f'pred_pt{ptchId}_{foldId}']\n",
    "          dfPredPatches[f'pred_pt{ptchId}'] /= (idx+1)\n",
    "          \n",
    "# print('Ensemble models prediction')\n",
    "# for modelName in ensembles.keys():\n",
    "#   ensModIsMax = False\n",
    "#   if modelName.split('_')[-1] in ['max','Max']:\n",
    "#     ensModIsMax = True\n",
    "#   pathPredFd =  F_PATH_PREDS(FOLDER)/f'{modelName}_fd.csv'\n",
    "#   pathPredPt =  F_PATH_PREDS(FOLDER)/f'{modelName}_pt.csv'\n",
    "#   if not pathPredPt.exists():\n",
    "#     for pathPred in [pathPredFd,pathPredPt]:\n",
    "#       predTag = pathPred.as_posix().split('_')[-1][:2]\n",
    "#       for idx,subModelName in enumerate(ensembles[modelName]):\n",
    "#         subModel = read_Dataframe_With_Dates(F_PATH_PREDS(FOLDER)/f'{modelDictRev[subModelName]}_{predTag}.csv')\n",
    "#         if idx == 0:\n",
    "#           dfPred = subModel.copy()\n",
    "#           pred_cols = [col for col in dfPred.columns if col[:3]=='pre']\n",
    "#           ptcPredCols = [col for col in dfPred.columns if len(col)==len('pred_pt0')]\n",
    "#           num_ptc = len(ptcPredCols)\n",
    "#         else:\n",
    "#           # keeping only common dates\n",
    "#           tmp = subModel.copy()\n",
    "#           tmp = tmp[tmp.index.isin(dfPred.index)].sort_index()\n",
    "#           dfPred = dfPred[dfPred.index.isin(tmp.index)].sort_index()\n",
    "#           # averaging\n",
    "#           if modelDictRev[subModelName].split('_')[2] == 'Persistant':\n",
    "#             # empirical persistent probability\n",
    "#             for ptchTag in [''] + [f'_pt{ptcId}' for ptcId in range(num_ptc)]:\n",
    "#               c = tmp[f'change{ptchTag}'].sum()\n",
    "#               tot = len(tmp)\n",
    "#               pChange = c/ (2*tot)\n",
    "#               tmp[tmp[f'histo{ptchTag}']==1][f'pred{ptchTag}'] = 1 - pChange # prob of positive event\n",
    "#               tmp[tmp[f'histo{ptchTag}']==0][f'pred{ptchTag}'] = pChange # prob of positive event\n",
    "#             for col in pred_cols:\n",
    "#               if ensModIsMax:\n",
    "#                 dfPred[col] = np.maximum(dfPred[col],tmp[col])\n",
    "#               else:\n",
    "#                 dfPred[col] = dfPred[col] + tmp[col]\n",
    "#           else:\n",
    "#             missingCols = [col for col in pred_cols if col not in tmp.columns]\n",
    "#             for col in missingCols:\n",
    "#               tmp[col] = dfPred[col]\n",
    "#             if ensModIsMax:\n",
    "#               for col in pred_cols:\n",
    "#                 dfPred[col] = np.maximum(dfPred[col],tmp[col])\n",
    "#             else:\n",
    "#               dfPred[pred_cols] = dfPred[pred_cols] + tmp[pred_cols]\n",
    "#       if not ensModIsMax:\n",
    "#         dfPred[pred_cols] = dfPred[pred_cols]/len(ensembles[modelName])\n",
    "#       dfPred = dfPred[['mpf','cls','label']+[col for col in dfPred.columns if col[:3]=='pre']]\n",
    "#       dfPred.to_csv(pathPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/greg/session/images')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pathCsvDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpathCsvDf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pathCsvDf' is not defined"
     ]
    }
   ],
   "source": [
    "pathCsvDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m F_PATH_TEST(\u001b[43mconfigTest\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelCol\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configTest' is not defined"
     ]
    }
   ],
   "source": [
    "F_PATH_TEST(configTest['labelCol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
